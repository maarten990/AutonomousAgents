\documentclass[a4paper, 12pt]{article}

\usepackage[english]{babel}
\usepackage{amsmath, amssymb}
\usepackage{parskip}

\begin{document}

\title{Single Agent Planning}
\author{Maarten de Jonge, Edwin Odijk, Ties van Rozendaal, Marysia Winkels}
\maketitle

\section*{Introduction}
This report describes the implementation of a simulation as required for the first assignment of the Autonomous Agents course at the University of Amsterdam. The simulation involves a "Predator" attempting to catch a "Prey" on a 11x11 toroidal grid, with the goal being for the Predator to end up on the same tile as the Prey.\\

% To Do: Van, nou ja, we hebben deze environment en deze agents en die kunnen dit en nu willen wij verschillende policies implementeren en deze vergelijken 
Policies considered for this simulation are:
\begin{itemize}
	\item Random policy: The predator has an equal probability of taking each
		available action, regardless of the state.
	\item Policy improvement: An optimal policy found by the policy improvement
		algorithm.
	\item Value iteration: An optimal policy found by the value iteration
		algorithm.
\end{itemize}

Simulations were run 100 times to ensure an accurate estimate of the average number of steps required to reach the goal, as well as the standard deviation.

\section*{Implementation} 
% (als in, in Python, wel/geen gebruik maken van klassen, en daarna over state representation)

\subsection*{State Representation}
The most naive state representation explicitly encodes the positions of both the
predator and the prey, giving it four parameters (the $x$ and $y$ coordinates of
the predator and the prey). The full state-space then consists of $N^4$ states,
where $N$ is the width and height of the world (assuming a square world for
simplicity).

As the policy evaluation algorithm has an outer loop iterating through the
entire state-space, it has a fairly unpleasant time complexity of
$\mathcal{O}(N^4)$. However, using the insight that the optimal policy only
depends on the relative positions of the predator and the prey, the size of the
state-space can be significantly reduced: when the state is encoded as only the
signed distance in the $x$ and $y$ directions between predator and prey, the
state space is reduced to $N^2$ states, and the policy evaluation algorithm is
then $\mathcal{O}(N^2)$. Quantitative performance measurements can be seen in table
\ref{tbl:performance}.

\section*{Comparison of Algorithms}
\subsection*{Efficiency}
As table \ref{tbl:sim_convergence} shows, there is no difference between the
policies produced by the policy improvement and the value iteration algorithm
(the slight differences in average and standard deviation are most likely due to the random
behaviour of the prey); unsurprisingly, both policies are a major improvement over the
random policy.

Table \ref{tbl:algo_convergence} shows the number of algorithm
iterations until the algorithm converges on the optimal policy; although value
iterations takes more iterations to converge, table \ref{tbl:performance} shows
that it still beats out policy improvement in terms of runtime, due to not
needing a full policy evaluation at each iteration.

\subsection*{Discount factor}
dickbutt

\begin{table}[htb]
	\begin{tabular}{|c|c|c|}
		\hline
		Algorithm & Average number of steps & standard deviation \\
		\hline
		Random policy & 262.69 & 188.80 \\
		Policy improvement & 11.26 & 2.27 \\
		Value iteration & 11.27 & 2.48 \\
		\hline
	\end{tabular}
	\caption{The number of time steps until convergence (i.e. the predator eats
		the prey) averaged over 100 trials, along with the standard deviation.}
	\label{tbl:sim_convergence}
\end{table}

\begin{table}[htb]
	\begin{tabular}{|c|c|c|c|c|c|}
		\hline
		Algorithm & $\gamma = 0.1$ & $\gamma = 0.2$ & $\gamma = 0.5$ & $\gamma =
		0.7$ & $\gamma = 0.9$ \\
		\hline
		Policy improvement & 5 & 8 & 3 & 3 & 3 \\
		Value iteration & 5 & 6 & 8 & 10 & 12 \\
		\hline
	\end{tabular}
	\caption{The number of iterations until the algorithm converges.}
	\label{tbl:algo_convergence}
\end{table}

\begin{table}[htb]
	\begin{tabular}{|c|c|c|}
		\hline
		Algorithm & Naive representation & Distance representation \\
		\hline
		Policy evaluation & 9.49 s (9490 ms) & 73.2 ms \\
		Policy improvement & 14.6 s (14600 ms) & 158 ms \\
		Value iteration & 7.51 s (75100 ms)& 132 ms \\
		\hline
	\end{tabular}
	\caption{The runtimes of various algorithms using both state representations.
		$\gamma = 0.8$ for all trials. Execution time has been averaged over 10
		runs, and the best time out of three separate trials is chosen.}
	\label{tbl:performance}
\end{table}

\section*{Conclusion}
As value iteration is significantly faster than policy improvement while
converging on the same policy, it is clearly the superior algorithm.
Something something discount factor.

\end{document}
