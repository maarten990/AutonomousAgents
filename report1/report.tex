\documentclass[a4paper, 12pt]{article}

\usepackage[english]{babel}
\usepackage{amsmath, amssymb}

\begin{document}

\title{Single Agent Planning}
\author{Ties van Rozendaal, Maarten de Jonge, Edwin Odijk, Marysia Winkels}
\maketitle

\section*{Comparison of Algorithms}
The following three policies have been considered:
\begin{itemize}
	\item Random policy: The predator has an equal probability of taking each
		available action, regardless of the state.
	\item Policy improvement: An optimal policy found by the policy improvement
		algorithm.
	\item Value iteration: An optimal policy found by the value iteration
		algorithm.
\end{itemize}

Todo: write about tables \ref{tbl:sim_convergence}, \ref{tbl:algo_convergence},
\ref{tbl:performance}.

\begin{table}[htb]
	\begin{tabular}{|c|c|c|}
		\hline
		Algorithm & Average number of steps & standard deviation \\
		\hline
		Random policy & 262.69 & 188.80 \\
		Policy improvement & 11.26 & 2.27 \\
		Value iteration & 11.27 & 2.48 \\
		\hline
	\end{tabular}
	\caption{The number of time steps until convergence (i.e. the predator eats
		the prey) averaged over 100 trials, along with the standard deviation.}
	\label{tbl:sim_convergence}
\end{table}

\begin{table}[htb]
	\begin{tabular}{|c|c|c|c|c|c|}
		\hline
		Algorithm & $\gamma = 0.1$ & $\gamma = 0.2$ & $\gamma = 0.5$ & $\gamma =
		0.7$ & $\gamma = 0.9$ \\
		\hline
		Policy improvement & 5 & 8 & 3 & 3 & 3 \\
		Value iteration & 5 & 6 & 8 & 10 & 12 \\
		\hline
	\end{tabular}
	\caption{The number of iterations until the algorithm converges.}
	\label{tbl:algo_convergence}
\end{table}

\begin{table}[htb]
	\begin{tabular}{|c|c|c|}
		\hline
		Algorithm & Naive representation & Distance representation \\
		\hline
		Policy evaluation & 9.49 s (9490 ms) & 73.2 ms \\
		Policy improvement & 14.6 s (14600 ms) & 158 ms \\
		Value iteration & 7.51 s (75100 ms)& 132 ms \\
		\hline
	\end{tabular}
	\caption{The runtimes of various algorithms using both state representations.
		$\gamma = 0.8$ for all trials. Execution time has been averaged over 10
		runs, and the best time out of three separate trials is chosen.}
	\label{tbl:performance}
\end{table}

\section*{State Representation}
The most naive state representation explicitly encodes the positions of both the
predator and the prey, giving it four parameters (the $x$ and $y$ coordinates of
the predator and the prey). The full state-space then consists of $N^4$ states,
where $N$ is the width and height of the world (assuming a square world for
simplicity).

As the policy evaluation algorithm has an outer loop iterating through the
entire state-space, it has a fairly unpleasant time complexity of
$\mathcal{O}(N^4)$. However, using the insight that the optimal policy only
depends on the relative positions of the predator and the prey, the size of the
state-space can be significantly reduced: when the state is encoded as only the
signed distance in the $x$ and $y$ directions between predator and prey, the
state space is reduced to $N^2$ states, and the policy evaluation algorithm is
then $\mathcal{O}(N^2)$. Quantitative performance measurements can be seen in table
\ref{tbl:performance}.

\end{document}
