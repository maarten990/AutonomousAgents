\documentclass[a4paper, 12pt]{article}

%\usepackage{natbib}
\usepackage[english]{babel}
\usepackage{amsmath, amssymb}
\usepackage{parskip}
\usepackage{graphicx}
\usepackage{url}

\begin{document}

\title{AA2 Practical Assignment}
\author{Maarten de Jonge, Koen Keune, Edwin Odijk,\\ Francesco Stablum, Marysia Winkels}
\maketitle

\section*{Introduction}
%something something sampling based fitted Q-iteration/value-iteration, something something continuous
This report describes the extension on an old AA1 assignment for the AA2 course. The existing predator/prey game's environment was changed from discrete to continuous, and the game was optimized using sampling based fitted Q-iteration/value-iteration.%\cite{ng_lecture}\cite{musze}

FVI (Fitted Value Iteration) is an algorithm that is suited for
continuous state spaces with continuos features extracted from
the state. 
The value function of a certain state is inferred by
extracting at first the features $\phi(s^{(i)})$ 
after having sampled some states 
$s^{(1)}\ldots s^{(i)} \ldots s^{(m)}$, 
and then by fitting via linear regression
the respective estimated state values $y^{(i)}$ 
in order to obtain the vector
of weights $\theta$.

The estimated state values $y^{(i)}$ are calculated as follows:
for each action $a$ and state $s^{(i)}$, 
$k$ state transitions are performed by using probabilities from 
an MDP model.
An estimated $Q$-value for the $\langle a,s^{(i)}\rangle$ pair is obtained
by averaging the sums of rewards of $s^{(i)}$ and
discounted value of the
state obtained from the transition from $s^{(i)}$.
In this step the $V$-function from the previous iteration has been used,
which, at the first iteration, has been set to 0.

These estimated $Q$-values are then used in a Bellman-like 
backup operator which selects the highest $Q$-value across all actions,
and for the specific current $s^{(i)}$.
This will become the current estimated $V$-value for $s^{(i)}$. 
This value is denoted
by the symbol $y^{(i)}$ in order to highlight the fact that it's 
a quantity that is subject to regression.

Once all $y^{(\cdot)}$ values have been obtained,
the previosly mentioned regression is performed.
This whole procedure is performed iteratively for a pre-determined
number of iterations.

\section*{Implementation} 
%HOW IT'S MADE
The algorithm has been implemented in one main function \texttt{fvi}
that resembles to a certain extent the pseudo-code
from Ng's lecture notes \cite{ng_lecture}.

Behavior has been parametrized by adding a number of lambda functions
as parameters to the function \texttt{fvi}:

\begin{itemize}
\item \texttt{sample\_states} is a function that returns the $s^{(1)}\ldots s^{(m)}$ sampled states
\item \texttt{sample\_actions} is a function that returns a set of sampled actions, which might be in principle sampled from a continuous distribution as well.
These action will then be used in order to calculate all $Q$-value
estimations for a certain state $s^{(i)}$.
\item \texttt{phi} is a function that will return a feature vector
$\phi(s^{(i)})$ for the state $s^{(i)}$ given as input.
A good feature vector representation is
crucial in order to obtain an acceptable regression.
\end{itemize}

One notable difference from Ng's version is that in our implementation
the probabilistic model of state transition is not stated explicitly
as a definition of a probability of transitioning to a new state 
given a previous state and an action, but is replaced by
just using a state transition function, whose implementation
was much easier and intuitive.

\section*{Comparison of Algorithms}
%new implementations vs old results
\includegraphics[scale=0.7]{convergence.png}
\includegraphics[scale=0.7]{n_samples.png}

\section*{Possible improvements}

Since the action space is continuous, different action sampling procedure
can be experimented. Moreover, different regressors for the 
$V$ function might be attempted,
such as \emph{Random Forests} or \emph{Support Vector Regression}.

The sampling of the states might also be focused on states that 
seem to be more probable, especially in the scenario of 
an online-focused variant
of \emph{FVI}. In the absence of such MDP probabilistic model,
easy-implementable techniques (with \texttt{sklearn}), such as
\emph{EM-Gaussian Mixture Models} can be fitted
after exploratory phases. Subsequently,
states might be sampled from the resulting \emph{GMM}.

\section*{Conclusion}
% ._.

\section*{References}
\bibliographystyle{plain}
\bibliography{ref}

\end{document}
